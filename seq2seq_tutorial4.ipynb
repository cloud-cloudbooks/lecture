{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = []\n",
    "korean = []\n",
    "count = 200\n",
    "with open('korean-english-park.train.en', 'r', encoding='utf8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        english.append(line)\n",
    "        if i-1 == count:\n",
    "            break\n",
    "\n",
    "with open('korean-english-park.train.ko', 'r', encoding='utf8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        korean.append(line)\n",
    "        if i-1 == count:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(english)):\n",
    "    english[i] = re.sub('\\n', '', english[i])\n",
    "for i in range(len(korean)):\n",
    "    korean[i] = re.sub('\\n', '', korean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(english)):\n",
    "    english[i] = english[i].split()\n",
    "for i in range(len(korean)):\n",
    "    korean[i] = korean[i].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_data = np.stack((english, korean), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_arr = []\n",
    "for seq in english:\n",
    "    word_arr += seq\n",
    "word_arr += ['<P>']\n",
    "en_word2num = {c:i for i, c in enumerate(set(word_arr))}\n",
    "en_num2word = {i:c for i, c in enumerate(en_word2num.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_arr = []\n",
    "for seq in korean:\n",
    "    word_arr += seq\n",
    "word_arr += ['<S>', '</S>', '<P>']\n",
    "ko_word2num = {c:i for i, c in enumerate(set(word_arr))}\n",
    "ko_num2word = {i:c for i, c in enumerate(ko_word2num.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(seq_data):\n",
    "    max_len = 0\n",
    "    for seq in seq_data:\n",
    "        if max_len < len(seq):\n",
    "            max_len = len(seq)\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(seq_data, enc_max_len, dec_max_len):\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "    target_batch = []\n",
    "    for i, seq in enumerate(seq_data):\n",
    "        input = []\n",
    "        output = []\n",
    "        target = []\n",
    "        for token in seq[0]:\n",
    "            input.append(en_word2num[token])\n",
    "        for _ in range(len(seq[0]), enc_max_len):\n",
    "            input.append(en_word2num['<P>'])\n",
    "        input_batch.append(input)\n",
    "        output.append(ko_word2num['<S>'])\n",
    "        for token in seq[1]:\n",
    "            output.append(ko_word2num[token])\n",
    "            target.append(ko_word2num[token])\n",
    "        target.append(ko_word2num['</S>'])\n",
    "        for _ in range(len(seq[1]), dec_max_len):\n",
    "            output.append(ko_word2num['</S>'])\n",
    "            target.append(ko_word2num['</S>'])\n",
    "                \n",
    "        output_batch.append(output)\n",
    "        target_batch.append(target)\n",
    "\n",
    "    return input_batch, output_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_length(seq_data):\n",
    "    seq_len = []\n",
    "    for i, seq in enumerate(seq_data):\n",
    "        seq_len.append(len(seq))\n",
    "    return seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "n_hidden = 128\n",
    "max_enc_step = get_max_length(english)\n",
    "max_dec_step = get_max_length(korean)\n",
    "n_embedding = 300\n",
    "total_epoch = 500\n",
    "batch_size = count\n",
    "en_dic_len = len(en_word2num)\n",
    "ko_dic_len = len(ko_word2num)\n",
    "n_input = len(en_word2num)\n",
    "n_class = len(ko_word2num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "enc_input = tf.placeholder(tf.int32, [None, max_enc_step])\n",
    "dec_input = tf.placeholder(tf.int32, [None, max_dec_step+1])\n",
    "dec_inputs = tf.one_hot(dec_input, ko_dic_len)\n",
    "W = tf.get_variable(name='embedding', shape=[en_dic_len, n_embedding], trainable=True)\n",
    "targets = tf.placeholder(tf.int64, [None, None])\n",
    "enc_seq_len = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "dec_seq_len = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "enc_inputs = tf.nn.embedding_lookup(W, enc_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('encode'):\n",
    "    enc_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "    #enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_inputs, sequence_length=enc_seq_len, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('decode'):\n",
    "    dec_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "    #dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_inputs, initial_state = enc_states, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(outputs, ko_dic_len, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.contrib.seq2seq.sequence_loss(logits=logits, targets=targets, weights=tf.sequence_mask(dec_seq_len+1, max_dec_step+1, dtype=tf.float32)))\n",
    "predict = tf.argmax(logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 050 cost = 0.133941\n",
      "epoch: 100 cost = 0.118241\n",
      "epoch: 150 cost = 0.094596\n",
      "epoch: 200 cost = 0.065003\n",
      "epoch: 250 cost = 0.040851\n",
      "epoch: 300 cost = 0.025782\n",
      "epoch: 350 cost = 0.016526\n",
      "epoch: 400 cost = 0.010770\n",
      "epoch: 450 cost = 0.006990\n",
      "epoch: 500 cost = 0.004653\n",
      "optimization finished!\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "total_batch = int(len(seq_data)/batch_size)\n",
    "for epoch in range(total_epoch):\n",
    "    loss_sum = 0\n",
    "    #print('< epoch:', epoch+1, '>')\n",
    "    for i in range(total_batch):\n",
    "        if i == (total_batch-1):\n",
    "            input_batch, output_batch, target_batch = make_batch(seq_data[i*batch_size:len(seq_data)], max_enc_step, max_dec_step)\n",
    "            enc_seq_data = get_seq_length(english[i*batch_size:len(seq_data)])\n",
    "            dec_seq_data = get_seq_length(korean[i*batch_size:len(seq_data)])\n",
    "        else:\n",
    "            input_batch, output_batch, target_batch = make_batch(seq_data[i*batch_size:(i+1)*batch_size], max_enc_step, max_dec_step)\n",
    "            enc_seq_data = get_seq_length(english[i*batch_size:(i+1)*batch_size])\n",
    "            dec_seq_data = get_seq_length(korean[i*batch_size:(i+1)*batch_size])\n",
    "        \n",
    "        _, loss = sess.run([optimizer, cost], feed_dict={enc_input: input_batch, dec_input: output_batch, targets: target_batch, enc_seq_len: enc_seq_data, dec_seq_len: dec_seq_data})\n",
    "        loss_sum += loss\n",
    "        #if i % 30 == 29:\n",
    "        #    print('batch:', '%03d' % (i+1), 'cost =', '{:.6f}'.format(loss_sum/30))\n",
    "        #    loss_sum = 0\n",
    "    if epoch % 50 == 49:\n",
    "        print('epoch:', '%03d' % (epoch+1), 'cost =', '{:.6f}'.format(loss_sum/50))\n",
    "        \n",
    "print('optimization finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(english):\n",
    "    english = [english]\n",
    "    korean = [['<P>']*max_dec_step]\n",
    "    seq_data = [english + korean]\n",
    "    input_batch, output_batch, target_batch = make_batch(seq_data, max_enc_step, max_dec_step)\n",
    "    enc_seq_data = get_seq_length(english)\n",
    "    dec_seq_data = get_seq_length(korean)\n",
    "    result = sess.run(predict, feed_dict={enc_input: input_batch, dec_input: output_batch, targets: target_batch, enc_seq_len: enc_seq_data, dec_seq_len: dec_seq_data})\n",
    "    decoded = [ko_num2word[j] for j in np.squeeze(result)]\n",
    "    end = len(decoded)-1\n",
    "    if '</S>' in decoded:\n",
    "        end = decoded.index('</S>')\n",
    "    translated = ' '.join(decoded[:end])\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Much of personal computing is about \"can you top this?\"  \n",
      "-> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 뛰어날 \n",
      "\n",
      "so a mention a few weeks ago about a rechargeable wireless optical mouse brought in another rechargeable, wireless mouse.  \n",
      "-> 모든 광마우스와 마찬가지 로 광마우스도 광마우스도 책상 위에 놓는 마우스 패드를 패드를 하지 \n",
      "\n",
      "Like all optical mice, But it also doesn't need a desk.  \n",
      "-> 그러나 이것은 또한 책상도 필요로 하지 않는다. \n",
      "\n",
      "uses gyroscopic sensors to control the cursor movement as you move your wrist, arm, whatever through the air.  \n",
      "-> 79.95달러하는 이 최첨단 무선 광마우스는 허공에서 팔목, 팔, 그외에 어떤 부분이든 부분이든 커서의 커서의 움직임을 \n",
      "\n",
      "Intelligence officials have revealed a spate of foiled plots on ships in Southeast Asia and are warning that a narrow stretch of water carrying almost one third of the world's maritime trade is vulnerable to a terror attack.  \n",
      "-> 정보 관리들은 동남 아시아에서의 선박들에 대한 많은 (테러) 계획들이 실패로 돌아갔음을 밝혔으며, 해상 해상 교역량의 거의 3분의 1을 운송하는 운송하는 좁은 말라카 말라카 해협이 테러 \n",
      "\n",
      "After learning of several foiled al Qaeda attempts on U.S. and commercial ships in the area, experts are warning that the terror network still wants to cripple the global economy, the world's economic jugular vein in Southeast Asia is at risk.  \n",
      "-> 이 지역에 있는 미국 선박과 상업용 선박들에 대한 알카에다의 (테러) 시도 시도 여러 건이 실패했다는 것을 알게 된 된 전문가들은 테러 테러 테러 경제에 입히려 입히려 입히려 입히려 있으며, 있으며, 동남 \n",
      "\n",
      "Caffeine can help increase reaction time and improve performance for military servicemen who must perform complex tasks or who need help staying alert for longer periods of time, according to a new report by the National Academy of Sciences.  \n",
      "-> 국립 과학 학회가 발표한 새 보고서에따르면, 복잡한 임무를 수행해야 수행해야 군인들이나 군인들이나 오랜 오랜 경계를 경계를 늦추지 위해 위해 위해 위해 위해 군인들에게 군인들에게 군인들에게 수행 향상시키는데 향상시키는데 한다. \n",
      "\n",
      "\"Specifically, it can be used in maintaining speed of reactions and visual and auditory vigilance, which in military operations could be a life or death situation,\" according to the report.  \n",
      "-> 이 보고서에따르면, \"특히, 군사 작전에서 생사가 걸린 상황이 상황이 수도 있는 반응 속도와 시각 시각 청각의 경계 상태를 유지시키기 카페인이 카페인이 카페인이 있다.\" 있다.\" \n",
      "\n",
      "\"Something that will boost their capabilities at crucial moments is very important.\"  \n",
      "-> \"결정적인 순간에 그들의 능력을 증가시켜 줄 그 무엇이 무엇이 \n",
      "\n",
      "Researchers are already exploring ways to put caffeine in nutrition bars or chewing gum as alternatives to coffee, Archibald said.  \n",
      "-> 연구가들이 이미 커피 대체품으로서 음식 대용 과자나 껌에 카페인을 첨가하는 방법을 연구하고 \n",
      "\n",
      "A similar dose of caffeine, about 200-600 mg, also appears effective in enhancing physical endurance and may be especially useful in returning some of the physical endurance lost at high altitude, the study found.  \n",
      "-> 그러나 200600밀리그램의, 비슷한 분량의 카페인은 또한 육체적 지구력을 강화시키는 데 데 같으며, 같으며, 같으며, 고도가 곳에서 약해진 약해진 육체적 지구력을 회복시켜주는 회복시켜주는 회복시켜주는 유용하다는 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, seq in enumerate(english):\n",
    "    temp = ''\n",
    "    for token in seq:\n",
    "        temp += token + ' '\n",
    "    #print(temp)\n",
    "    print(temp, '\\n->', translate(seq), '\\n')\n",
    "    if i == 10:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tensor",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
